**Speak2Sign** is an AI-powered system that translates **spoken English** into **American Sign Language (ASL)** grammar using a combination of **speech recognition** and **natural language processing**.

It supports:
- ğŸ—£ï¸ **Real-time transcription** using Whisper or Vosk
- ğŸ¤ **Audio or video file uploads**
- ğŸ§  **ASL grammar conversion** via a fine-tuned Flan-T5 transformer model
- ğŸ–¼ï¸ **Animated sign visualization** using pre-generated ASL GIFs

---

## ğŸš€ Features

- ğŸ”Š Live or recorded audio input
- âœï¸ Converts English into ASL gloss (grammar)
- ğŸ¤– Fine-tuned Flan-T5 model for grammar transformation
- ğŸï¸ Visual sign animation (GIF-based, easily extendable)
- ğŸŒ Clean web UI (Bootstrap + Flask)
- âš¡ GPU-supported training & inference

---

## ğŸ§  How It Works

1. **Speech Transcription**: Converts live or uploaded audio into English using Whisper/Vosk.
2. **ASL Grammar Converter**: Translates English to ASL grammar using a fine-tuned Flan-T5 model.
3. **Animation Display**: Displays word-by-word ASL sign GIFs in the web interface.

---

## ğŸ“ Project Structure

```
speak2sign/
â”œâ”€â”€ training/                   # Fine-tuning script for Flan-T5
â”œâ”€â”€ inference/                 # Inference pipeline and model comparison
â”œâ”€â”€ data/                      # ASL dataset (from ASLG-PC12 or custom)
â”œâ”€â”€ scripts/                   # Dataset scraping and formatting
â”œâ”€â”€ bart_model/                # Your previous BART-based pipeline
â”œâ”€â”€ models/asl_flan_t5/        # Saved fine-tuned model
â”œâ”€â”€ static/asl_animations/     # ASL GIFs
â”œâ”€â”€ templates/index.html       # Bootstrap frontend
â”œâ”€â”€ app.py                     # Flask app for UI + API
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## ğŸ”§ Installation

### âœ… Prerequisites

- Python 3.8+
- pip
- Optional: CUDA-compatible GPU

---

### ğŸ“¦ Set up environment

```bash
# 1. Clone the repo
git clone https://github.com/YOUR_USERNAME/speak2sign.git
cd speak2sign

# 2. Create virtual environment (optional but recommended)
python -m venv env
source env/bin/activate       # On Windows: env\Scripts\activate

# 3. Install dependencies
pip install -r requirements.txt
```

---

### ğŸ”¡ Download and Prepare the Dataset

```bash
python scripts/prepare_aslg_pc12.py
```
This script will:
- Download the ASLG-PC12 dataset
- Format it into `data/asl_data.csv`

---

### ğŸ§  Fine-tune the Flan-T5 Model

```bash
python training/train_flan_t5_asl.py
```
The model will be saved to `models/asl_flan_t5/`

---

### ğŸ§ª Run Side-by-Side Comparison (BART vs Flan-T5)

```bash
python inference/compare_models.py
```

---

### ğŸŒ Run the Web App

```bash
python app.py
```

Then open [http://127.0.0.1:5000](http://127.0.0.1:5000) in your browser.

---

## ğŸ–¼ï¸ ASL GIFs

- Save your ASL sign animations (e.g., `hello.gif`, `thank_you.gif`) in:
```
static/asl_animations/
```
- Filenames must match the ASL gloss words in your model output.

---

## ğŸ§ª Example Inputs/Outputs

| English (Input)             | ASL Gloss Output         |
|-----------------------------|--------------------------|
| What is your name?          | YOUR NAME WHAT           |
| I am going to the store.    | I GO STORE               |
| Can you help me?            | YOU HELP ME CAN          |

---

## ğŸ› ï¸ Tech Stack

- ğŸ¤– Transformers: `google/flan-t5-base`
- ğŸ”Š Whisper / Vosk for audio transcription
- ğŸ§ª Hugging Face Datasets + Trainer
- ğŸ–¥ï¸ Flask + Bootstrap frontend
- âš™ï¸ Torch, SentencePiece, Pandas

---

## ğŸ™Œ Acknowledgements

- [ASLG-PC12 Dataset](https://achrafothman.net)
- [Flan-T5 Model (Google)](https://huggingface.co/google/flan-t5-base)
- [Whisper Speech-to-Text (OpenAI)](https://github.com/openai/whisper)

---

## ğŸ“œ License

This project is MIT licensed. Please credit datasets and models used in your own deployments.

---

## ğŸ’¬ Questions or Contributions?

Feel free to open an issue or PR. Contributions are welcome!
